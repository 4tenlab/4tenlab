{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/4tenlab/4tenlab/blob/main/Youtube_%EC%8A%A4%ED%81%AC%EB%A6%BD%ED%8A%B8_%EC%B7%A8%ED%95%A9_%ED%94%84%EB%A1%9C%EA%B7%B8%EB%9E%A8%EC%9D%98_%EC%82%AC%EB%B3%B8.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Youtube script를 다운로드 받기 위한 파이썬 모듈을 설치합니다.\n",
        "!pip install youtube-transcript-api"
      ],
      "metadata": {
        "id": "w-eN1raFOt45",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83c7f066-3a48-411d-91bc-c75a3c6e3719"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting youtube-transcript-api\n",
            "  Downloading youtube_transcript_api-0.6.2-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from youtube-transcript-api) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->youtube-transcript-api) (2024.7.4)\n",
            "Downloading youtube_transcript_api-0.6.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: youtube-transcript-api\n",
            "Successfully installed youtube-transcript-api-0.6.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from youtube_transcript_api import YouTubeTranscriptApi\n",
        "\n",
        "def extract_video_id(url):\n",
        "    # YouTube URL에서 동영상 ID를 추출하는 정규식 패턴\n",
        "    pattern = r\"(?<=v=)[\\w-]+(?=&|\\s|$)\"\n",
        "    match = re.search(pattern, url)\n",
        "    if match:\n",
        "        return match.group()\n",
        "    else:\n",
        "        return None"
      ],
      "metadata": {
        "id": "9mjkllnGQk7i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCF2ToQiOkO_"
      },
      "outputs": [],
      "source": [
        "# Youtube_URL을 아래에 변경해서 입력하세요.\n",
        "youtube_url = \"https://www.youtube.com/watch?v=O4RoqVTPvSc\"\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 동영상 ID 추출\n",
        "video_id = extract_video_id(youtube_url)\n",
        "\n",
        "# 영문 혹은 한국어로 제작된 유튜브의 경우 스크립트를 가져올 수 있음.\n",
        "data=YouTubeTranscriptApi.get_transcript(video_id,languages=('en','ko'))\n",
        "\n",
        "# 'text' 필드만 추출하여 하나의 문자열로 합침\n",
        "texts = [entry['text'] for entry in data]\n",
        "result = ' '.join(texts)\n",
        "\n",
        "# 결과 출력\n",
        "print(result)"
      ],
      "metadata": {
        "id": "q8yJUbIaQnMZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82b84f25-7b30-4f9e-a144-cc9c969fdf2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "안녕하세요 tx AI 클럽입니다 제가 지금 출장 때문에 울산에 내려와 있는데 어 좀 꽤 밤이어서 큰 소리를 못 내기 때문에 어 잘 소리를 좀 키워 주시면은 잘 들리실 것 같아요 그래서 일단 오늘 어 어떤 내용들을 다룰 건지 먼저 설명을 해 드리자면 어 제가 이제 바로 전 영상에서 어 메타에서 출시한 라마 3 모델에 대해서 좀 어 정도 설명을 해 드렸는데 그거를 이제 업로드를 한 그날 당시에 마이크로소프트에서 이제 좀 더 작은 버전의 모델인 파이 3 모델을 출시를 했어요 그래서 오늘 영상에서는 그 둘의 모델들의 성능들을 비교를 할 건데 그걸 이제 비교하기 위해서 좀 바로 성능 테스트해 볼 수 있는 코드를 같이 좀 작성을 해보고 걔를 이제 테스트해보면서 어떻게 답변하는 같이 좀 살펴볼 예정입니다 그래서 그 코드를 를 작성하기 위해서는 저희가 두 가지 정 종류의 어 프레임워크 언어 패키지들을 사용을 할 수가 있어요 그래서 제가 이제 지금이 슬라이드에서 보시면은 제가 주로 항상 사용하는 어 프레임워크 툴킷은 이제 랭 체인이라는 패키지입니다 그래서 어 원래는 항상 뭔가 lml 사용한 앱들을 만들기 위해서는이 랭 체인이라는 툴을 사용을 했었는데 오늘은이 랭 체인 말고 라마라 거를 사용을 해 볼 거예요 그래서이 둘의 차이점에 대해서 좀 설명을 드리자면 일단 공통점은이 두 개를 둘 다 사용을 해서 어 내가 사용하고자 하는 언어 모델들을 좀 바로 그냥 이렇게 호출해서 사용할 수 있게끔 좀 개발자들이 좀 편하게 쉽게 사용할 수 있게끔 만든 이런 인프라 툴이라고 생각을 하시면 되고요 그 둘의 차이점에 대해서 설명을 해 드리자면 랭 체인은 조금 더 어 굉장히 다양한 키들을 좀 되게 편리하게 사용할 수 있는 좀 더 자유도가 높은 프레임워크라고 생각을 하시면 돼요 그래서 저희가 오픈 AI 아니면 뭐 라마 3나 이런 다양한 제미나이 뭐 이런 모델들을 사용하는이 앱을 만들고자 할 때이 앱을 조금 더 편리하게 좀 다양하게 자유도 높게 만들고 싶다 하신 분들은 랭 체인을 사용하시면 되고요 오라마 같은 경우에는 똑같 같은 이런 프레 프레임워크 툴이지만 얘는 조금 더 검색하는 기능에 특화된 이런 언어 패키지라고 생각을 하시면 됩니다 그래서 제가 이렇게 적어 놓은 것처럼 어 이렇게 검색을 하고 내가 필요한 내용들을 찾아주는 그런 기능에 조금 더 맞춤이 되어 있는 패키지이기 때문에 만약에 나는 뭐 다른 그냥 어 이런 창의적인 뭐 기능들이라 이런건 필요 없고 그냥 딱 효율적이고 빠르게 검색하는 기능만 좀 빨리 테스트해 보고 싶다 하시는 분들은 라마를 사용해 보시는 거를 추천드립니다 그래서 오늘은이 라마라 거를 사용을 해서 라마 3랑 그리고 파이 3의 모델들을 같이 이제 비교를 해보면서 테스트를 해 볼 거예요네 어 시작하기에 앞서 일단은 그냥 조금 더 큰 틀에서 라마 3랑 파이 3의 차이점에 대해서 좀 설명을 해 드릴 건데요 일단 어 라마 3는 제 그까 전 영상에서도 좀 어느 정도 설명을 해 드렸어요 그래서 얘는 현재 이제 작은 버전의 모델이 이제 80억 개의 파라미터를 학습을 한 거고 얘는 이제 메타에서 만들었고요 뭐 최근에 이제 뭐 다른 조금 더 작은 소형 사이즈의 언어 모델들인 뭐 미리나 줌마보 이제 기능을 뛰어넘고 있다라고 벤치마크 숫자들이 이제 나온 상태고 그리고 얘는 이제 다른 나라 언어들도 좀 어느 정도 학습을 학습이 되어 있는 상태고 이 코드와 관련된 데이터들을 사전에 많이 학습했기 때문에 뭐 코드를 뭐 고쳐주거나 작성해 주는 이런 기능들은 다른 뭐 언어 모델들보다 얘가 이제 더 잘 해주고 있다라는 그런 후기가 나오고 있고요 얘가 원래 그 입력 값인 그 컨텍스트 윈도 얼마나 많은 입력값을 넣을 수 있냐 그게 이제 굉장히 작았는데 그거를 이제 다시 뭐 파인 튜닝을 해서 좀 더 많은 양의 입력 값들을 넣을 수 있는 그런 파인 튜닝된 다른 모델들이 지금 계속 나오고 있으니까 그걸 한번 확인해 보시는 걸 추천드리고 최근에 뭐 한국어 특화된 이런 파인 튜닝된 라마 3 모델도 나오고 있어서 테스트해 보시는 거를 추천드리고요 얘랑 이제 비교를 해서 파이 3에 대해서 좀 설명을 드리자면 얘는 이제 마이크로소프트에서 만든 소형 slm이라고 해요 그래서 원래 저희가 지금 잘 알고 있는 거는 llm 이잖아요 그래서 뭐 대자 어 큰 또 자연 언어 모델 이런 거라고 생각을 하면 되는데 얘는 이제 훨씬 더 작은 slm이라고 하는 small lang 모이라고 합니다 얘는 이제 마이크로소프트에서 만들었고 얘는 이제 라마 3보다 훨씬 더 작은 38억 개의 파라미터가 학습이 되었고요 얘 말고도 지금 파이 3 뭐 미니도 있고 얘보다 조금 더 많은 파라미터가 학습이 되어 있는 조금 더 큰 버전의 모델들도 같이 지금 나온 상태입니다 얘는 어 훨씬 더 모델이 작기 때문에 어 훨씬 더 작은 그 로컬 환경의 모바일에서도 얘를 돌릴 수가 있어요 그래서 그거에 대한 논문도 나와 있고 어 하지만 이제 한 가지 단점은 뭐냐면 얘가 이런 파라미터 수가 좀 그렇게 많지 않다 보니까 얘도 이제 사전에 학습할 수 있는 데이터의 양이 제한되어 있을 수밖에 없어요 그래서 어 기능적인 한계가 있을 수밖에 없고 지금 몇 가지 테스트를 해 보니까 다른 나라 언어 특히나 뭐 한국어나 일본어 같은 거 는 얘가 이제 잘 뭐 이해를 못 해 주거나 잘 못해 주는 경우가 있어서 그런 단점이 있다고 생각을 하시면 됩니다 그래서 정말 그냥 기본적으로 큰 틀에서 좀 이런 차이점이 있다라고 생각을 하시면 되고 얘는 이제 GPT 3.5 다 기능을 뛰어넘고 있다라고 벤치마크 결과들이 나오고 있습니다 그래서이 벤치마크의 숫자들을 좀 비교를 하자면 지금 이제 방금 설명드린게이 라마스 3 어 80억 개 파라미터란 그리고 가장 작은 파 3 미니 버전에 38억 개의 파라미터가 학습되어 있는이 작은 모델들이 있고요 파이 3도 마찬가지로 조금 더 큰 버전에 파이 3 스몰이 있고 파이 3 미디엄도 있습니다 그래서 얘네들의 벤치마크를 지금 비교를 해보면이 mml 아는 거는 조금 더 긴 어 롱폼 형태의 굉장히 많은 글들을 이해 얼마나 잘 이해할 수 있는가 그 벤치마크 라고 생각을 하시면 됩니다 그래서이 숫자를 지금 보시면은 GPT 3.5 다 라마 3가 뛰어넘고 있고요 파이 3는 GPT 3.5 조 비슷한 그런 벤치마크 숫자가 나왔고 이제 여기서 MT 벤츠라는 거는 MA translation 벤츠라는 건데 얘는 어이 특정 언어를 뭐 다른 나라 언어로 얼마나 잘 번역을 하는가이 벤치마크 있고 얘도 지금 보시면은 라마 3는 GPD 3.5 다 뛰어넘고 있고 파이 3미 도 GPT 3.5 거의 비슷한 그런 형태라고 보시면 되고요 뭐 파 3 스이나 미디엄은 GPT 3.5를 뛰어넘고 있는 거를 확인하실 수가 있습니다 그래서 저희가 지금까지 좀 간략하게 정말 큰 틀에서이 둘의 모델들을 비교를 해봤고요 얘를 이제 바로 테스트 하기 위해서 제가 작성한 코드를 같이 한번 살펴볼게요네 일단은 그 두 개의 모델들의 성능 테스트를 하기 전에 제가 이제 아까 방금 설명드린게 그 라마랑 랭 체인 프레임워크의 차이점을 알려 드렸잖아요 근데 조금 더 구체적으로 실제로 코드로 봤을 때이 둘이 좀 어떻게 다른지 한번 먼저 살펴보고 나서 그리고 그 둘의 모델들을 비교하는 코드를 보여 드릴게요 일단은 사실 구조상으로 랭 CE이나 오 라마나 거의 똑같아요 어 아 정정하겠습니다 라마가 아니라 라마 인덱스라는 거를 사용을 할 거예요 그래서 라마 인덱스 랭 체인은 구조상 거의 똑같아요 왜냐하면 저희가 특정 뭐 언어 모델을 사용을 해서 앱을 만들고자 할 때 그 데이터를 뭐 불러오고 저장하고 이러는 관계는 다 똑같기 때문에 그 구조는 다 똑같기 때문에 실제로 구현되는이 코드도 둘 다 같습니다 그래서 일단은 그 두 개의 모델들의 성능들을 테스트하기 전에 일단 제가 아까 설명드린 그 두 개의 프레임워크 있잖아요 랭 체인이랑 라마 인덱스가 있다고 했어요 그래서 일단은 얘는 이제 랭 체인의 구조이고 얘에 대해서 그냥 짧게 설명을 해 드리자면 저희가 지금 얘는 뭐냐면 어 저희가 이제 학습 시키고자 하는 PDF 파일이나 텍스트 파일을 업로드를 한 다음에 걔를 이제 그래서이 단계가 지금이 첫 번째 단계예요 그래서 PDF 뭐 텍스트 파일을 업로드를 했어요 그러면은 컴퓨터가 이해를 하기 위해서는이 단어들을 다시 이제 작은 단위로 쪼갠 다음에 그거를 벡터라는 숫자로 다시 바꿔 줘야 돼요 그래서 첫 번째 단계인이 PDF 파일들에 있는 내용들을 작은 단위로 쪼개 주는 거를 텍스트 슬리터라인 크 단위로 이렇게 자른 다음에 걔네들을 이제 오픈 AI 임베딩이 뭐 다른 이제 다양한 형태의 임베딩 함수들이 있어요 그래 걔네를 사용을 해서 벡터 숫자로 바꿔주는 그 단계를 임베딩이 합니다 그래서이 임베딩으로 변환을 해 준 다음에 그 각 단어들의 뭐 각 문장들의 임베딩 값들을이 데이터베이스 벡터 DB 아는 거에다가 저장을 하게 됩니다 그래 그래서 지금 이제 설명을 해 줬을 때이 임베딩 값들을 생성해 주는 굉장히 다양한 임베딩 제너레이터 함수들도 있고요 그 생성된 임베딩을 저장해 주는이 데이터베이스도 굉장히 다양한 여러 개가 있습니다 그래서 뭐 파인콘 말고도 크로마 티비라는 것도 있고 아니면 뭐 파이스라 것도 있고 굉장히 많습니다 그래서 데이터베이스의 그 임베딩 값들을 다 업로드를 해 줘요 그러고 나서 사용자는 이제 쿼리를 합니다 그래서 쿼리는 뭐냐면 그 업로드한 텍스트나 PDF 파일에 대해서 내가 검색하고자 하는 거를 얘가 이제 질문을 던지는 거죠 그러면 그 질문을이 컨텍스트에 맞게 같이 합쳐 준 다음에 아 나는 이런 식으로 여기 내가 업로드한 파일들 중에서 이거를 좀 찾으면 되겠구나라는 그래서 그 질문을 어 인간처럼 이해를 하고 그거에 맞게 만약에 얘가 답변을 찾아냈으면 그 답변을 생성해주는 거를 언어 모델들이 해주는 겁니다 그래서 GPT 3나 GPT 4나 뭐 라마나 파 3나 제미나이 이런 거를 다 사용을 해서 얘가 이제 답변을 생성을 한 다음에 사용자한테 주는 거예요 그래서 이게 체인의 기본 구조입니다 만약에 저희가 라마 인덱스 구조를 한번 살펴봤어요 얘도 방금 제가 설명드린 거랑 똑같아요 그래서 your 데이터라는 부분이 제가 이제 학습 시키고자 하는 뭐 PDF 파일이나 텍스트 파일 그래서 데이터베이스나 뭐 다큐멘트 뭐 API 관련된 다 뭐 정형화된 데이터 구조도 되고 비정형화된 데이터도 되고 아니면 코드어 돼요 이거를 다 얘가 이해할 수 있도록 인덱스라는 임베딩 값으로 변환을 해주고요 걔를 이제 마찬가지로데 어 데이터베이스에 저장을 업로드를 해서 저장을 해 준 다음에 사용자가 마찬가지로 질문을 보내거나 뭐 뭐를 찾아줘 이거는 뭐 어떤 거야라고 물어보면 얘가이 llm 모델을 사용을 해서 그 질문에 대한 답변을 찾아내고 그리고 그 답변을 생성해서 얘가 사용자한테 다시 주는 그 과정을 뜻합니다 그래서 지금 방금 설명한이 단계를 저희가 코드로 한번 살펴 볼게요 그래서 저희가 이제 코드로 살펴볼 건데요 지금 이제이 파일을 보시면은 얘는 랭 체인의 기본적인 구조들을 지금 코드로 변환을 해 준이 코드고 진짜 거의 뭐 50줄 밖에 안 돼서 굉장히 쉽고 지금이 라마 인덱스로 되어 있는 파일이 얘는 이제 라마 인덱스 프레임워크를 사용을 해서 방금 설명드린 그 구조를 한번 살펴볼 겁니다 일단은 먼저 어 체인에 대해서 설명 을 해 드릴게요 그래서 이거는 제 전 영상들에서 저는 이제 거의 다 랭 체인을 항상 사용해 왔던 사람이어서 체인은 저한테 굉장히 이제 쉽게 다가오기도 하고 몇 번 이제 보셨을 것 같은데 일단은이 단계별로 똑같이 설명을 해 드릴게요 저희가 뭐 몇 가지 패키지를 다운받아야 돼서 뭐 패키지 다운받는 거는 여기 위에서 그냥 하시면 되고요 지금 여기서 저희는 오픈 AI 사용할 거기 때문에 어 오픈 AI API 키를 를 불러와 줘야 돼요 그래서이 과정에서 제가 비밀번호를 따로이라는 파일에 저장을 했고 걔를 지금 여기서 사용하도록 불러온 겁니다 그래서 내 환경 설정에서 오픈 a API 키를 불러와라 그 과정을 여기서 하고 있는 거고요 저희는 이제 어 아까 이제 답변을 생성을 할 때 llm 모델이 필요하다고 했잖아요 그래서 여기서 라마 3 모델을 사용을 해서 답변을 생성하도록 할 겁니다 그래서 랭 체인에서 되게 여러 가지의 언어 모델들을 사용할 수 있도록 얘네가 이렇게 프레임워크를 구축을 해 줬어요 저는 그 중에서 라마를 사용을 하시면 라마 3나 파이 3나 아니면 뭐 위드나 뭐 뭐 미스 이런 애들을 다 사용할 수 있어서 저는 라마 패키지를 사용을 할 거고 그 중에서 모델이 라마 3 모델을 사용할 거다라고 알려 주는 겁니다 그래서 여기서 만약에 저희가 뭐 파이 3를 사용하고 싶 다 그럼 이런 식으로 그냥 작성을 해주면 돼요 저희는 라마 3를 일단 사용을 하겠습니다 그래서 언어 모델을 일단 여기서 구축을 해줬고요 그리고 첫 번째 단계인 이제 얘한테 학습시킬 파일을 여기서 알려주는 겁니다 그래서 저희가 데이터를 로딩을 해 줄 거예요 그래서이 단계가 지금 딱이 단계입니다 그래서 PDF 파일을 업로드를 하는 로딩을 해주는 단계 그래서 그게 여기서 랭 체인을 통해서 저희는 웹베이스 로더는 거를 사용을 할 건데 얘는 웹사이트에 있는 html 있는 텍스트 데이터를 내가 추출해 오겠다 가능하게끔 하는 그런 패키지입니다 지금 보시면은 제가 링크 하나를 첨부를 했어요 그래서이 링크는 지금 뭐 사과주스의 효능에 대해서 설명을 하고 있는 위키피디아 페이지인데 걔를 이제이 링크를 여기다 넣으면은 이거를 사용을 해서 얘가 그 안에 있는 내용들을 로딩을 하겠다라고 알려 주는 겁니다 그러면 얘가 이제 로딩을 했으니까 제가 아까 그 청크 단위로 쪼은 다음에 걔를 임베딩으로 바꿔 줘야 된다고 했잖아요 그래서 그 과정을 지금이 오픈 AI 임베딩을 사용을 해서 얘네를 임베딩 할 거다라고 이렇게 지금 불러온 겁니다 그다음으로 저희가 데이터를 쪼개 줄 거예요 그래서이 텍스트 슬리터라인 해서 방금 위에서 로딩한이 위키피디아 디 페이지에 있는 텍스트를 작은 단위로 쪼 쪼갠다라는 과정을 여기서 하고 있습니다 데이터를 쪼개어 그러고 나서 얘를 데이터 쪼갠 거를 이제 임베딩 값이랑 같이 이렇게 합쳐서이 벡터 데이터베이스에 업로드를 해 주는 겁니다 그래서 이제이 과정이 되는 거겠죠이 임베딩 값을 방금 이제 텍스트를 쪼개고 임베딩 값을 바꿔 줘서 걔를 이제 데이터베이스 에 업로드를 한다라고 이해를 하시면 돼요 그래서 데이터베이스에 업로드를 했고 이제 여기서 저희가 만약에 사용자가 질문을 보냈을 때 그거에 대한 답변을 찾고 답변을 생성하게 하는 거를 LM 모델을 사용을 해서 답변을 생성하도록 해야 돼요 그래서 여기서 이제 저희가 프롬프트를 작성을 해 줍니다 너는 뭐 어떤 식으로 대답을 해 줘라고 말해 주는 그런 프롬프트를 작성을 해 주는 거예요 그래서 그냥 영어로 작성되어 있긴 한데 그냥 사용자가 질문을 하면 너는이 컨텍스트에 맞게 네가 대답을 해 줘라라고 알려주는 프롬프트 있니다 그래서이 프롬프트를 사용을 해서 저희가 위에서 이렇게 알려준이 llm이 지금 라마 3를 사용을 하고 있잖아요 그래서 그 llm이랑이 프롬프트를 합쳐서 저희가이 다큐먼트 체인이라는 LM 체인을 생성을 하게 됩니다 그래서 체인 은 이런 식으로 계속 하나의 체인을 만들고 다른 체인을 만들고 그 둘의 체인을 합칠 수도 있고 계속 이렇게 체인들이 연속 시킬 수 있다라는 그런 기능들이 있고요 어쨌든 저희는 이제 이런 다큐멘트 체인을 만들었기 때문에 그다음으로 해야 되는게 뭐냐면 저희가 검색을 하고 나서 그 검색에 대한 값들 어 값을 가져올 수 있는 리브 할 수 있는 그리브 체인도 만들어 줘야 돼요 그래서 여기서 업로드한 데이터를 불러올 수 있는 이런 체인을 만든 다음에 여기서 이제 저희가 질문을 합니다 얘를 이제 호출을 하는 거예요 뭐 사과 주스는 건강에 어떻게 좋은가요라고 이렇게 물어보면 얘가 이제 답변을 하게 될 거예요 얘를 지금 테스트를 한번 해 보면 얘가 이제이 모든 과정들을 거치고 있는 거예요 그래서 그 위키피디아 페이지에 있는 데이터를 로딩 을 하고 쪼갠 다음에 벡터 스토어에 업로드를 하고 사용자가 질문을 했으니까 그거에 대한 답변을 찾고 답변을 생성하는 과정까지 얘가 진행을 하게 됩니다 그래서 좀 기다리면네 그러면 얘가 이제 답변을 해줬어요 그래서 보시면은 사과 주수는 뭐 이런 식으로 건강의 여러 가지 효능이 있다 뭐 영양 강화 뭐 좋은 살균 뭐 뭐 비타민씨가 좋고 뭐 이런 식으로 얘가 있 이제 답변을 생성을 하게 되었습니다 그래서 지금 이제 살펴본 거는 랭 체인의 구조를 살펴봤고요 저희가 이제 두 번째로 살펴볼 거는이 똑같은 거를 라마 인덱스로 어떻게 하는가를 살펴볼게요 그래서 라마 인덱스는 딱이 구조 그냥 거의 똑같아요 그래서이 구조가 어떻게 코드로 구현되어 있는지 한번 살펴보겠습니다 그래서 지금 보시면은 얘는 이제 다른 필요한 패키지들을 지금 일단 호출을 해 왔고 어 저희가 여기서 보실 때 데이터를 로딩하는 과정에서는 뭐 만약에 웹사이트 데이터를 로딩하고 싶다 그러면 웹베이스 로더는 거를 사용을 해야 되고 만약에 PDF 로딩하고 싶다 그러면은 뭐 PDF 로더는 이런게 있어요 그래서 각 그거에 맞는 데이터 형식에 맞는 패키지를 사용을 해야 되는데 얘는 그냥로드 데이터를 하시 면은 얘가 이제이 함수를 사용을 해서 저희가 업로드한 데이터를 바로 불러올 수가 있습니다 얘는 지금이 데이터라는 폴더에 따로 저장되어 있는이 텍스트 파일을 지금 얘가 로딩을 할 거예요 이거는 폴 그레이엄의 뭐 인생사에 대한 뭐 에세이 텍스트인 얘를 이제 호출을 해서 저희는 데이터를 로딩을 했습니다 얘를 이제 데이터 로딩을 하면은 저희는 이제 아까는 그 임베딩 값을 얘는 이제 다시 바꿔 줘야 되잖아요 근데 아까는 오픈 AI 임베딩을 사용을 했다면 이제는 오라마에서 지원하고 있는 임베딩 모델들 중에서도 굉장히 여러 가지가 있습니다 그래서 저희가 이제 원하는 임베딩 함수를 선택을 해 주시면 되고요 저는 그 중에서 지금이 노믹 임베딩 모델이라는 거를 사용을 하고 있습니다 그래서 환경 설정 자체에 나는이 임베딩 모델을 사용할 거다라고 알려주면 됩니다 그러고 나서 저희가 아까 이제 랭 체인을 살펴볼 때에는이 LM 모델을 활성화해 줬잖아요 그래서 이맨 위에서 LM 라마 그래서 라마 3를 사용을 했는데 똑같이 이제 저희는이 llm 세팅을 해 줄 때 라마를 활용을 해서 라마 3라는 모델을 사용할 거다라고 이렇게 알려주고 있습니다네 그러고 나서 저희가이 만든 임베딩 값들을 벡터 스토어에가 이렇게 업로드를 해 주면 돼요 그래서 그 인덱스라는게 이제 생긴 겁니다 그러면이 인덱스를 사용을 해서 검색 엔진을 얘를 검색 엔진으로 사용을 해라라고 알려 주는 겁니다 그래서이 인덱스 저희가 만든 임베딩 값들을 벡터 스토어에 업로드를 했으니까 얘를 이제 검색 엔진으로 사용해라 이렇게 알려주고 있는 거고요 지금 여기서 similar 케는 어 사용자가 질문을 했을 때 그거에 대한 답변을 찾아요 근데 얘가 굉장히 여러 가지의 답변들을 찾는 방법이 뭐냐면 얘랑 이제 관련성이 높은 여러 가지 답변들을 얘가 내놓는데 그중에서 가장 유사성이 높은 점수가 가장 높은 애만 한 가지만 가져와라라고 알려주고 있는 겁니다 그래서이 유사성이 가장 높은 답변을 네가 불러와 줘라고 알려주는 거예요 그러면 이제 저희가 검색 엔진을 만들었으니까 그러면 어떤 식으로 답변하도록 해야 되는지 그 프롬프트를 여기서 작성을 해 줍니다 그래서 그냥 마찬가지로 너는 뭐 이런 식으로 답변을 해 줘라고 알려 주는 거고요 얘를 이제 사용을 해서이 검색 엔진에이 프롬프트를 탑재시킨 다음에 저희가 바로 이제 질문을 할 수가 있습니다 그래서 로그 은 뭐 잘할 때 어떤 일들을 뭐 어떤 활동들을 했나요라고 물어볼 수가 있는 거요 그래서 얘한테 똑같이 한번 실행을 해 보면 얘도 똑같이 라마 3를 사용을 해서 답변을 하게 됩니다네 그러면 얘는 이제 라마 인덱스를 사용을 해서 임베딩 값들을 지금 생성을 하고 있고 그 중에서 유사성이 가장 높은 답변 첫 번째 거를 불러 오게 될 거예요네 그러면 얘가 이제 저희가 학습시킨 그 데이터에 맞게 답변을 찾고 답변을 생성하게 됩니다 그래서 이런 식으로 어 라마 인덱스 지금 랭 체인이 어떻게 구성되어 있는지 그 코드에 대해서 설명을 해 드렸고요 지금 시간이 좀 많이 지났기 때문에 저희가 다음 영상에서 바로이 라마 인덱스를 사용을 해서 어 라마 3랑 파 3를 비교해 보는 거를 같이 한번 해 볼 거예요 그래서 그거를 어떻게 구성되어 있는지 잠깐 보여 드리자면 이런 식으로 구성이 되어 있습니다 그래서 파이 3나 라마 3 모델을 선택할 수가 있고 PDF 추가한 다음에 얘한테 똑같이 질문을 몇까지 던져 볼 거예요 그래서이 각 모델이 어떻게 답변하는 한번 테스트를 해 보겠습니다 그래서 다음 영상도 빠르게 업로드를 할테니까 오늘은 일단 기본적으로 어 라마 인덱스 그리고 랭 체인의 차이 점이랑 그리고 그 두 가지 모델들의 차이점에 대해서 정말 그냥 간단하게만 설명을 해 드렸어요네 그래서 오늘 좀 크게 말할 수 없어서 되게 작게 말했는데 만약에 이제 좋은 정보가 되셨다면 구독과 좋아요까지 부탁드리겠습니다 감사합니다\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 결과를 파일에 저장\n",
        "with open(\"output.txt\", \"w\", encoding=\"utf-8\") as file:\n",
        "    file.write(result)\n",
        "\n",
        "print(\"결과가 output.txt 파일에 저장되었습니다.\")"
      ],
      "metadata": {
        "id": "bCeHoTv2QhFy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b66760bd-5d90-4071-d65c-98194796f6b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "결과가 output.txt 파일에 저장되었습니다.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.download(\"output.txt\")"
      ],
      "metadata": {
        "id": "dB6FkDrERiOq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "624a5f25-c67c-45dd-dc51-d5ee52b16321"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4654bb83-94a1-4fbb-8f2c-fb015de5a2d7\", \"output.txt\", 24852)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}